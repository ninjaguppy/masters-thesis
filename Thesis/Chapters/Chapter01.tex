%************************************************
\chapter{A First Attempt}\label{ch:FirstAttempt}
%************************************************


\section{Introduction}%
\label{sec:Intro}

As a note to the reader (and myself): things written in {\color{blue} blue}
denote things I want to add/expand upon, things writteng in {\color{red} red}
denote things that I need to add/find out/fix, and things in {\color{green} green}
denote wording I don't like but want to come back to

{\color{blue} Gotta do this at some point. Maybe here we define things like \(\mathcal{U}_n\).}

\section{Functional Calculus}%
\label{sec:functionalcalc}

Functional Calculus refers to the process of extending the domain of a function
on \(\RR\) to include matrices (or in some cases operators). The most basic
formulation uses the fact that the space \(n \times n\) matrices forms a ring
and so there is a natural way to evaluate polynomials \(f \in \CC [x]\). If we
require that $A \in M_n(\CC)$ is self-adjoint---and hence diagonalizable as
$A = U \Lambda U^*$---then it is a standard result that:
\begin{align*}
  f(A) &= a_nA^n + \cdots + a_1A + a_0 I_n \\
  &= a_n \left( U\Lambda U^* \right) ^n + \cdots + a_1 U\Lambda U^* + a_0 I_n \\
  &= a_n U\Lambda^n U^* + \cdots + a_1 U\Lambda U^* + a_0 I_n \\
  &= U \left( a_n\Lambda ^n + \cdots + a_1\Lambda + a_0 I_n \right) U^* \\
  &= U \left( f(\Lambda) \right) U^*
\end{align*}
Further, since \(\Lambda\) is diagonal and $f$ is a polynomial,
\[
  f \left( \begin{bmatrix} \lambda_1 &  &  \\  & \ddots &  \\  &  & \lambda_n \end{bmatrix}  \right)
  = \begin{bmatrix} f(\lambda_1) &  &  \\  & \ddots &  \\  &  & f(\lambda_n) \end{bmatrix}
\]
Therefore, given a self-adjoint matrix \(A\) and a polynomial \(f \in \CC [x]\)
\[
  f(A) = Uf(\Lambda)U^* = U \;\text{diag}\{f(\lambda_{1}), \dots , f(\lambda_n)\} \; U^*
\]
Notice that can simply substitute \(A\) in for \(x\) without any trouble as long
as we transform the constant term \(a_0 \mapsto a_0I_n\) when evaluation on
\(n\times n\) matrices.\footnote{Technically we have
  \(a_0 \mapsto a_0 \otimes I_n\) but they are identical in this case. It is common in free
  analysis to tensor by \(I_n\) to {\color{red} make matrices the same size}.}
Since self-adjoint matrices play such a vital role in free analysis, we will let
\(\HH_n \subset M_n(\CC)\) denote the set of \(n \times n\)-matrices over \(\CC \).
With the polynomial case in mind, we can extend a function \(g: [a,b] \to \CC \)
to a function on self adjoint matrices with their
spectrum in \([a,b]\). Let \(A\) be such a matrix (diagonalized by the unitary
matrix \(U\)), and define
\[
  g(A) = U
  \begin{bmatrix} g(\lambda_1) & &\\ &\ddots& \\ & & \lambda_n \end{bmatrix}
  U^*
\]
Thus, for each \(n \in \NN \), \(g\) induces a function on the self-adjoint
\(n \times n\) matrices with spectrum in \([a,b]\).  The natural ordering
 on self-adjoint matrices is called the
\textbf{Loewner Order}:

\begin{definition}[Loewner Ordering]
\label{def:LoewnerOrder}
  For like size self-adjoint matrices, we say that \(A \preceq B\) if \(B - A \)
  is positive semidefinite and \(A \prec B\) is \(B-A\) is positive definite.
\end{definition}

With this ordering in place, we can extend many of the familiar function
theoretic properties (monotonicity, convexity) to these matrix-values functions.
In fact, these properties are defined identically to their classical counterpart:
We say that a function is \emph{matrix-monotone} if \(A \preceq B\) implies that
\(f(A) \preceq f(B)\) and \emph{matrix-convex} (or \emph{nc-convex}) if
\[
  f \left( \frac{X+Y}{2} \right) \preceq \frac{f(X)+f(Y)}{2}
\]
for every pair of like-size matrice for which \(f\) is defined. These condition
are rather restrictive (since the must hold for matrices of \emph{all} sizes) so
many functions which are convex/monotone (in the traditional sense) fail to be
matrix-convex/monotone. For a full treatment of nc-convexity, see
\cite{heltonFree2013}. To illustrate the restrictiveness of nc-convexity,
{\color{red} we will steal an example from Helton. (I don't like this phrasing. I don't
  mind the word steal, its just awkward)}
\begin{example}%
\label{ex:helton1}
  In contrast to the real (or even complex) case, \(f(x)=x^4\) fails to be nc-convex.
  Indeed, if
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}
  \]
  Then
  \[
    \frac{X^4+Y^4}{2} - \left( \frac{1}{2}X +\frac{1}{2}Y \right) ^4
    = \begin{bmatrix} 164 &120\\120&84 \end{bmatrix}
  \]
  Which is not positive definite! Thus \(x^4\) fails to be convex on even
  \(2\times 2\) matrices.
\end{example}

Further, a number of the standard constructions lift identically in this
functional calculus.
\begin{definition}[Directional Derivative]
  \label{def:DirDeriv}
  The derivative of \(f\) in the direction \(H\) is
  \[
    Df(X)[H] = \lim_{t \to 0} \frac{f(X+tH) - f(X)}{t}
  \]
  where \(H\) and \(X\) are like-size self-adjoint matices.
\end{definition}

Often, the best way to compute these directional derivatives is via an
equivalent formulation:
\[
    Df(X)[H] = \left.\frac{df(X+tH)}{dt}\right|_{t=0}
\]
This version allows us to more easily define higher order derivatives
\[
    D^{(k)}f(X)[H] = \left.\frac{d^{(k)}f(X+tH)}{d^{(k)}t}\right|_{t=0}
\]

\begin{example}
  Just as in the classical case, the directional derivative is linear, so we
  will only show a calculation of a monomial. Let \(f(x)=x^3\). Since \(X\) and
  \(H\) do not commute,
  \begin{align*}
    f(X+tH) &= X^3+ tX^2H +tXHX + t^2XH^2\\
    & \hphantom{= X^3}+ tHX^2 + t^2HXH + t^2H^2X +t^3H^3.
  \end{align*}
  From here, we can calculate:
  \begin{align*}
    \frac{d}{dt} f(X+tH) &= X^2H + XHX + 2tXH^2 +HX^2\\
                  &\hphantom{=X^2H} +2tHXH +2tH^2X + 3t^2H^3 \\
     \\
    \frac{d^2}{dt^2} f(X+tH) &= 2XH^2+2HXH +2H^2X + 6tH^3 \\
     \\
    \frac{d^3}{dt^3} f(X+tH) &= 6H^3.
  \end{align*}
  And so the first 3 directional derivatives are:
  \begin{align*}
    Df(X)[H] &= X^2H + XHX +HX^2\\
    \\
    D^{(2)}f(X)[H] &= 2XH^2+2HXH +2H^2X \\
    \\
    D^{(3)}f(X)[H] &= 6H^3
  \end{align*}
  In general, the \(k\)-th derivative of a polynomial is degree \(k\) as a
  polynomial in \(H\).
\end{example}

Just as in the classical case, the second derivative tells gives us information
about the convexity of a function. A function \(f:M_n(\CC)\to M_n(\CC)\) is said
to be \textbf{positive} if \(0\preceq A \implies 0\preceq f(A)\). In the
functional calculus, we say that \(f\) is \textbf{nc positive} if it is positive
as a map on \(M_n(\CC)\) for all \(n\).
Despite nc-convexity being so restrictive, Lemma 12 in \cite{heltonFree2013}
shows that the standard characterization of convexity via the second
derivative: a function \(f\) is convex if and only if \(D^{2}f(X)[H]\) is
nc-positive. Unlike the classical case, however, the only convex polynomials are
of degree 2.\footnote{See \cite{heltonFree2013} for details.}

\section{Extending Multi-Variable Functions}%
\label{sec:ExtMuliVarFun}

We can extend this same functional calculus to functions of several variables,
although the details are a bit more subtle. We could simply ``plug in'' at tuple
of matrices to a standard multivariable polynomial ring over \(\RR \) or
\(\CC \), but this ignores the noncommutativity of \(M_n(\CC)\). In light of this,
let \(x = (x_1 , \dots x_g)\) be a
\(g\)-tuples of noncommuting formal variables. The formal variables
\(x_1, \dots , x_n\) are \emph{free} in the sense that there are no nontrivial
relations between them.\footnote{This becomes important in the eventual
  functional calculus---matrices \emph{do} have nontrivial relations. See
  section [ALGEBRAIC CONSTRUCTION] for the details.}
A \textbf{word} in \(x\) is a product of these
variables (\eg \(x_1x_3x_1x_4^2\) or \(x_1^2x_5^3\)). An \textbf{nc-polynomial}
in \(x\) is a formal finite linear combination of words in \(x\) with
coefficients in your favorite field. We use \(\RR \langle x \rangle\) and
\(\CC \langle x \rangle\) to denote the set of nc-polynomials in \(x\) over
\(\RR \) or \(\CC \) respectively.

With \(\CC \langle x \rangle \) constructed, we can define the functional
calculus. Given a word \(w(x) = x_{i_1}^{p_1}\cdots x_{i_d}^{p_d}\) and a
\(g\)-tuple of self-adjoint matrices, \(X\), we can evaluate \(w\) on \(X\) via
\(w(X) = X_{i_1}^{p_1}\cdots X_{i_d}^{p_d}\). Since our nc-polynomials are
linear combinations of these words, we can extend this evaluation to evaluation
of entire polynomials. Algebraically, we have a natural evaluation map:
Given some \(f \in \CC \langle x \rangle \) and
\(X = \left( X_1, \dots ,X_g \right) \) a
\(g\)-tuple of self-adjoint matrices, define
\begin{align*}
  \varepsilon_f: {\HH_\bullet}^g &\longrightarrow M_\bullet(\CC) \\
             X &\longmapsto f(X)
.\end{align*}
Notice that our functions are \textbf{graded} in the sense that if \(X\) is a
tuple of \(n \times n\) matrices, then \(f(X)\) is also a tuple of
\(n \times n\) matrices.

\begin{example}%
\label{ex:multivareval}
  Let \(f(x,y) = x^2-xyx +1 \in \RR \langle x,y \rangle  \). If we define
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}
  \]
  as before, then
  \begin{align*}
    f(X,Y) &= X^2 - XYX + I_2 \\
           &= \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}^2
             -\begin{bmatrix} 4 &2\\2&2 \end{bmatrix}\begin{bmatrix} 2&0\\0&0 \end{bmatrix}\begin{bmatrix} 4 &2\\2&2 \end{bmatrix}
             +\begin{bmatrix} 1&0\\0&1 \end{bmatrix} \\
           &= \begin{bmatrix} -11&-4\\-4&1 \end{bmatrix}.
  \end{align*}
  Additionally,
  \begin{align*}
    f(X\oplus X,Y\oplus Y) &= \begin{bmatrix} -11&-4&0&0\\-4&1&0&0 \\ 0&0&-11&-4 \\ 0&0&-4&1 \end{bmatrix} \\
    &= f(X,Y) \oplus f(X,Y).
  \end{align*}
  It is no accident that polynomials handle direct sums of matrices well. As in
  the classical case, they are the ``well behaved'' example which we would like
  general objects to emulate. In the next chapter, we will define
  free functions---which behave like nc polynomials.
\end{example}

In the context of these multivariate functions, our definition of the
Directional Derivative still makes sense (although our direction \(H\) now
becomes a tuple of directions). We also inherit (from multi-variable calculus) a
notion of the \textbf{gradient} of a function---but this will require a bit more work.

\subsection{The Natural Involution on nc-Polynomials}%
\label{ssec:NatInvo}

Given our ring of nc polynomials, we may define an involution \(^*\) which we
may view as an extension of the conjugate transpose. Let \(^*\) reverse the
order of words (\ie \((x_1x_3x_2^2)^* = x_2^2x_3x_1\)) and extend linearly to
all of \(\RR \langle x \rangle \). We consider the formal
variables \(x_1, \dots , x_n\) \emph{symmetric} in the sense that
\(x_i^* = x_i\). We say that a polynomial \(p \in \RR \langle x \rangle \) is
symmetric if \(p^* = p\). For example, if
\[
  p(x) = 5x_1^2x_3x_2 + x_3x_2x_3 \qquad q(x) = 3x_2x_1x_2 + x_3^2 - x_1 ,
\]
then a cursory inspection tells that \(q\) is symmetric while \(p\) is not.

Notice that the majority of the previous two sections breaks down if we try to
extend functions to non-self-adjoint matrices. The act of ``plugging in''
a tuple of arbitry matrices to some element of \(\RR \langle x \rangle \) via the same
functional calculus described above still works, but \(\RR \langle x \rangle \)
is no longer the natural algebra for these evaluations.

Let \(x = (x_1, \dots, x_g)\) be formal variables and let
\(x^* = (x_1^*, \dots, x_g^*)\) denote their formal adjoints. Once again, we let
the ring \(\RR \langle x,x^* \rangle \) be the finite formal sums of words in
\(x_1,x_1^*, \dots , x_g,x_g^*\) with coefficients in \(\RR \). Endow
\(\RR \langle x,x^* \rangle \) with an involution \(^*\) which sends
\(x_i \mapsto x_i^*\) and \(x_i^* \mapsto x_i\) and reverses the order of words
extended linearly. Notice that this involution behaves identically to the
adjoint with respect to products and sums of matrices. This new ring inherits a
natural functional calculus just like that in section \ref{sec:ExtMuliVarFun}
except it can accept \emph{any} matrix as an input instead of simply
self-adjoint matrices.

\begin{example}
  Let \(f(x,y) = x^*y - x y^* x +2\). Then
  \[
    f^*(x,y) = y^*x - x^*yx^* + 2.
  \]
  Evaluating \(f\) on a pair of non self-adjoint matrices is left to the reader.
\end{example}

\subsection{Matrices of nc-Polynomials}

It is occasionally useful in the larger theory of free analysis (\eg{} when
construction the free topology in section \ref{sec:admtopo}) to consider
matrices where the matrices are nc polynomials. Formally,
let \(\RR \langle x \rangle ^{k\times k}\) denote the set of \(k \times k\) matrices
with entries in \(\RR \langle x \rangle \).\footnote{Some sources additionally
  consider non-square matrices but this is rare.}
We can naturally extend the involution \(^*\) on \(\RR \langle x \rangle \) to
our matrices by applying \(^*\) component wise and taking the transpose of the
matrix.\footnote{We could likewise define
  \(\RR \langle x,x^* \rangle\) and extend the corresponding involution.}

Given some \(\delta \in \RR \langle x \rangle ^{k \times k}\) a matrix of nc
polynomials, and \(X \in \HH_n^g\) there is a natural evaluation map.
\begin{align*}
  \varepsilon_\delta: {\HH_n}^g &\longrightarrow M_{nk}(\CC) \\
             X &\longmapsto \delta(X)
\end{align*}
given by evaluating each polynomial in \(\delta\) at \(X\) and then viewing the
result at a block \(k \times k\) where each block is an \(n \times n\) matrix.

\begin{example}
  Define \(\delta \in \RR \langle x,y \rangle ^{2\times 2}\) as
  \[
    \delta(x,y) = \begin{bmatrix}
               x^2-xyx+1 & xy-yx \\
               x^4 & y^3-5xy+3
        \end{bmatrix}
  \]
  Then
  \[
    \delta^*(x,y) = \begin{bmatrix}
               x^2-xyx+1 & yx-xy \\
               x^4 & y^3-5yx+3
        \end{bmatrix}
  \]
  For an evaluation, we will once again let
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}.
  \]
  We already know what the evaluations of the first column from examples
  \ref{ex:helton1} and \ref{ex:multivareval}, so we need only complute the
  second column.
  \begin{align*}
    XY-YX &= \begin{bmatrix} 0&-4\\4&0 \end{bmatrix}  \\
    \\
    Y^3-5XY+3 &= \begin{bmatrix} -29&0\\-20&3 \end{bmatrix}
  \end{align*}
  And thus
  \[
    \delta(X,Y) =
    \begin{bmatrix}
      -11&-4&0&-2 \\
      -4&1&4&0 \\
      164&120&-29&0\\
      120&84&-20&3
    \end{bmatrix}.
  \]
\end{example}
