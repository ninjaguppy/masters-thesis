%************************************************
\chapter{A First Attempt}\label{ch:FirstAttempt}
%************************************************

The fields of free analysis and noncommutative function theory are in their
(comparative) infancy. Seeking to understand functions of noncommuting
indeterminants, so-called ``free functions'' have a natural evaluation on
tulpes of matrices. In this thesis, we introduce the basic objects and maps that
underly free analysis---\cref{ch:FirstAttempt} first does this through
considering
polynomials and \cref{ch:SecondAttempt} generalizes the work
of chapter 1 to include more exotic functions. Then in, \cref{pt:two}, we explore
some of the major results in generalizing algebraic geometry and topology.


Before we get into the meat, a few preliminary pieces of notation:
 \(\mathcal{U}_n\) will denote the set of \(n\times n\) unitary matrices. Unless
otherwise stated, all maps are morphisms in their respective categories.
\footnote{Those fearful of category theory need not despair, this thesis only
  invokes a universal property once.}

\section{Functional Calculus}%
\label{sec:functionalcalc}

Functional Calculus is the process of extending the domain of a function
on \(\RR\) to include matrices (or in some cases operators). The most basic
formulation uses the fact that the space \(n \times n\) matrices forms a ring
and so there is a natural way to evaluate polynomials \(f \in \CC [x]\). If we
require that $A \in M_n(\CC)$ is self-adjoint---and hence diagonalizable as
$A = U \Lambda U^*$---then it is a standard result that:
\begin{align*}
  f(A) &= a_nA^n + \cdots + a_1A + a_0 I_n \\
  &= a_n \left( U\Lambda U^* \right) ^n + \cdots + a_1 U\Lambda U^* + a_0 I_n \\
  &= a_n U\Lambda^n U^* + \cdots + a_1 U\Lambda U^* + a_0 I_n \\
  &= U \left( a_n\Lambda ^n + \cdots + a_1\Lambda + a_0 I_n \right) U^* \\
  &= U \left( f(\Lambda) \right) U^*
\end{align*}
Further, since \(\Lambda\) is diagonal and $f$ is a polynomial,
\[
  f \left( \begin{bmatrix} \lambda_1 &  &  \\  & \ddots &  \\  &  & \lambda_n \end{bmatrix}  \right)
  = \begin{bmatrix} f(\lambda_1) &  &  \\  & \ddots &  \\  &  & f(\lambda_n) \end{bmatrix}
\]
Therefore, given a self-adjoint matrix \(A\) and a polynomial \(f \in \CC [x]\)
\[
  f(A) = Uf(\Lambda)U^* = U \;\text{diag}\{f(\lambda_{1}), \dots , f(\lambda_n)\} \; U^*
\]
Notice that can simply substitute \(A\) in for \(x\) without any trouble as long
as we transform the constant term \(a_0 \mapsto a_0I_n\) when evaluating on
\(n\times n\) matrices.\footnote{Technically we have
  \(a_0 \mapsto a_0 \otimes I_n\) but they are identical in this case. It is common in free
  analysis to tensor by \(I_n\) to make the matrices compatible.}
Since self-adjoint matrices play such a vital role in free analysis, we will let
\(\HH_n \subset M_n(\CC)\) denote the set of \(n \times n\) self adjoint matrices over \(\CC \).
With the polynomial case in mind, we can extend a function \(g: [a,b] \to \CC \)
to a function on self adjoint matrices with their
spectrum in \([a,b]\). Let \(A\) be such a matrix (diagonalized by the unitary
matrix \(U\)), and define
\[
  g(A) := U
  \begin{bmatrix} g(\lambda_1) & &\\ &\ddots& \\ & & g(\lambda_n) \end{bmatrix}
  U^*
\]
Thus, for each \(n \in \NN \), \(g\) induces a function on the self-adjoint
\(n \times n\) matrices with spectrum in \([a,b]\). Many properties of functions
of a single real variable utilize the fact that \(\RR \) is totally ordered.
While there is not a total ordering on \(\HH \), there is a partial ordering;
the natural ordering on self-adjoint matrices is called the
\textbf{Loewner Order}:

\begin{definition}[Loewner Ordering]
\label{def:LoewnerOrder}
  For like size self-adjoint matrices, we say that \(A \preceq B\) if \(B - A \)
  is positive semidefinite and \(A \prec B\) is \(B-A\) is positive definite.
\end{definition}

With this ordering in place, we can extend many of the familiar function
theoretic properties (monotonicity, convexity) to these matrix-valued functions.
In fact, these properties are defined identically to their classical counterpart.
We say that a function is \emph{matrix-monotone} if \(A \preceq B\) implies that
\(f(A) \preceq f(B)\) and \emph{matrix-convex} (or \emph{nc-convex}) if
\[
  f \left( \frac{A+B}{2} \right) \preceq \frac{f(A)+f(B)}{2}
\]
for every pair of like-size matrices, \(A\) and \(B\), for which \(f\) is defined. These condition
are rather restrictive (since the must hold for matrices of \emph{all} sizes) so
many functions which are convex/monotone (in the traditional sense) fail to be
matrix-convex/monotone. For a full treatment of nc-convexity, see
\cite{heltonFree2013}. To illustrate the restrictiveness of nc-convexity,
consider the following example.
\begin{example}%
\label{ex:helton1}
  In contrast to the real (or even complex) case, \(f(x)=x^4\) fails to be nc-convex.
  Indeed, if
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}
  \]
  Then
  \[
    \frac{X^4+Y^4}{2} - \left( \frac{1}{2}X +\frac{1}{2}Y \right) ^4
    = \begin{bmatrix} 164 &120\\120&84 \end{bmatrix},
  \]
  which is not positive definite! Thus \(x^4\) fails to be convex on even
  \(2\times 2\) matrices.
\end{example}

Further, a number of the standard constructions lift identically in this
functional calculus.
\begin{definition}[Directional Derivative]
  \label{def:DirDeriv}
  The derivative of \(f\) in the direction \(H\) is
  \[
    Df(X)[H] := \lim_{t \to 0} \frac{f(X+tH) - f(X)}{t}
  \]
  where \(H\) and \(X\) are like-size self-adjoint matices.
\end{definition}

Often, the best way to compute these directional derivatives is via an
equivalent formulation:
\[
    Df(X)[H] = \left.\frac{df(X+tH)}{dt}\right|_{t=0}
\]
This version allows us to more easily define higher order derivatives
\[
    D^{(k)}f(X)[H] = \left.\frac{d^{(k)}f(X+tH)}{d^{(k)}t}\right|_{t=0}
\]
Notice that this formulations requires each derivative to be in the same
direction. Higher order derivatives in different directions clearly exist
(simply nest the limits), but their complexity grows quickly. For example, the
second derivative in two directions, first \(H\) and then \(K\) is given by
\[
  D^2f(X)[H][K] = \lim_{t \to 0} \frac{Df(X+tK)[H] - Df(X)[H]}{t}.
\]

\begin{example}
  Just as in the classical case, the directional derivative is linear, so we
  will only show a calculation of a monomial. Let \(f(x)=x^3\). Since \(X\) and
  \(H\) do not commute,
  \begin{align*}
    f(X+tH) &= X^3+ tX^2H +tXHX + t^2XH^2\\
    & \hphantom{= X^3}+ tHX^2 + t^2HXH + t^2H^2X +t^3H^3.
  \end{align*}
  From here, we can calculate:
  \begin{align*}
    \frac{d}{dt} f(X+tH) &= X^2H + XHX + 2tXH^2 +HX^2\\
                  &\hphantom{=X^2H} +2tHXH +2tH^2X + 3t^2H^3 \\
     \\
    \frac{d^2}{dt^2} f(X+tH) &= 2XH^2+2HXH +2H^2X + 6tH^3 \\
     \\
    \frac{d^3}{dt^3} f(X+tH) &= 6H^3.
  \end{align*}
  And so the first 3 directional derivatives are:
  \begin{align*}
    Df(X)[H] &= X^2H + XHX +HX^2\\
    \\
    D^{(2)}f(X)[H] &= 2XH^2+2HXH +2H^2X \\
    \\
    D^{(3)}f(X)[H] &= 6H^3
  \end{align*}
  In general, the \(k\)-th derivative of a polynomial is degree \(k\) as a
  polynomial in \(H\).
\end{example}

Just as in the classical case, the second derivative gives us information
about the convexity of a function. A function \(f:M_n(\CC)\to M_n(\CC)\) is said
to be \textbf{positive} if \(0\preceq A \implies 0\preceq f(A)\). In the
functional calculus, we say that \(f\) is \textbf{nc positive} if it is positive
as a map on \(M_n(\CC)\) for all \(n\).
Despite nc-convexity being so restrictive, Lemma 12 in \cite{heltonFree2013}
shows that the standard characterization of convexity via the second
derivative: a function \(f\) is convex if and only if \(D^{2}f(X)[H]\) is
nc-positive. Unlike the classical case, however, the only convex polynomials are
of degree 2.\footnote{See \cite{heltonFree2013} for details.}

\section{Extending Multi-Variable Functions}%
\label{sec:ExtMuliVarFun}

We can extend this same functional calculus to functions of several variables,
although the details are a bit more subtle. We could simply ``plug in'' a tuple
of matrices to a standard multivariable polynomial ring over \(\RR \) or
\(\CC \), but this ignores the noncommutativity of \(M_n(\CC)\).

For example, consider \(p \in \CC [x,y]\) defined by
\[
  p(x,y) = xy = yx.
\]
If we were to evaluate \(p\) on \(X,Y \in \HH^2\), should it be
\[
  p(X,Y) = XY, \;\; P(X,Y)=YX, \textrm{ or } p(X,Y) = \frac{XY+YX}{2}?
\]

It is evident, then, that \(C[x_1, \dots,x_n]\) is not the algebra of
polynomials that we should use.
In light of this,
let \(x = (x_1 , \dots x_g)\) be a
\(g\)-tuples of noncommuting formal variables. The formal variables
\(x_1, \dots , x_n\) are \emph{free} in the sense that there are no nontrivial
relations between them.\footnote{This becomes important in the eventual
  functional calculus---matrices \emph{do} have nontrivial relations. We solve
  this by creating so-called ``generic matrix'' rings. \cite{klepPositive2018}
  and \cite{klepTracepositive2011}
  contain the construction as well as a list of further sources.}
A \textbf{word} in \(x\) is a product of these
variables (\eg \(x_1x_3x_1x_4^2\) or \(x_1^2x_5^3\)). An \textbf{nc polynomial}
in \(x\) is a formal finite linear combination of words in \(x\) with
coefficients in your favorite field. We use \(\RR \langle x \rangle\) and
\(\CC \langle x \rangle\) to denote the set of nc-polynomials in \(x\) over
\(\RR \) or \(\CC \) respectively.

With \(\CC \langle x \rangle \) constructed, we can define the functional
calculus. Given a word \(w(x) = x_{i_1}^{p_1}\cdots x_{i_d}^{p_d}\) and a
\(g\)-tuple of self-adjoint matrices, \(X\), we can evaluate \(w\) on \(X\) via
\(w(X) = X_{i_1}^{p_1}\cdots X_{i_d}^{p_d}\). Since our nc-polynomials are
linear combinations of these words, we can extend this evaluation to evaluation
of entire polynomials. Algebraically, we have a natural evaluation map:
Given some \(f \in \CC \langle x \rangle \) and
\(X = \left( X_1, \dots ,X_g \right) \) a
\(g\)-tuple of self-adjoint matrices, define
\begin{align*}
  \varepsilon_f: {\HH_\bullet}^g &\longrightarrow M_\bullet(\CC) \\
             X &\longmapsto f(X)
.\end{align*}
Notice that our functions are \textbf{graded} in the sense that if \(X\) is a
tuple of \(n \times n\) matrices, then \(f(X)\) is an
\(n \times n\) matrices.

\begin{example}%
\label{ex:multivareval}
  Let \(f(x,y) = x^2-xyx +1 \in \RR \langle x,y \rangle  \). If we define
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}
  \]
  as before, then
  \begin{align*}
    f(X,Y) &= X^2 - XYX + I_2 \\
           &= \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}^2
             -\begin{bmatrix} 4 &2\\2&2 \end{bmatrix}\begin{bmatrix} 2&0\\0&0 \end{bmatrix}\begin{bmatrix} 4 &2\\2&2 \end{bmatrix}
             +\begin{bmatrix} 1&0\\0&1 \end{bmatrix} \\
           &= \begin{bmatrix} -11&-4\\-4&1 \end{bmatrix}.
  \end{align*}
  Additionally,
  \begin{align*}
    f(X\oplus X,Y\oplus Y) &= \begin{bmatrix} -11&-4&0&0\\-4&1&0&0 \\ 0&0&-11&-4 \\ 0&0&-4&1 \end{bmatrix} \\
    &= f(X,Y) \oplus f(X,Y).
  \end{align*}
\end{example}
  It is no accident that polynomials handle direct sums of matrices well. As in
  the classical case, they are the ``well behaved'' example which we would like
  general objects to emulate. In the next chapter, we will define
  free functions---which behave like nc polynomials.

In the context of these multivariate functions, our definition of the
Directional Derivative still makes sense (although our direction \(H\) now
becomes a tuple of directions). We also inherit (from multi-variable calculus) a
notion of the \textbf{gradient} of a function---but this will require a bit more work.

\subsection{The Natural Involution on nc Polynomials}%
\label{ssec:NatInvo}

Given our ring of nc polynomials,\footnote{For the remainder of this chapter,
  we will work with \(\RR \langle x \rangle \), but an identical construction
  holds for \(\CC \langle x \rangle \).}
we may define an involution \(^*\) which we
may view as an extension of the conjugate transpose. Let \(^*\) reverse the
order of words (\ie \((x_1x_3x_2^2)^* = x_2^2x_3x_1\)) and extend linearly to
all of \(\RR \langle x \rangle \). We consider the formal
variables \(x_1, \dots , x_n\) \emph{symmetric} in the sense that
\(x_i^* = x_i\). We say that a polynomial \(p \in \RR \langle x \rangle \) is
symmetric if \(p^* = p\). For example, if
\[
  p(x) = 5x_1^2x_3x_2 + x_3x_2x_3 \qquad q(x) = 3x_2x_1x_2 + x_3^2 - x_1 ,
\]
then a cursory inspection tells that \(q\) is symmetric while \(p\) is not.

Consider the case of plugging in non-self-adjoint matrices. We can
still evaluate polynomials on arbitrary matrices, but the involution described
above is no longer appropriate. Since the involution is analogous to taking the
conjugate transpose after evaluation, if we are evaluating on arbitrary
matrices, our variables should distinguish between \(x_i\) and \(x_i^*\). Thus,
if we want to allow evaluation on arbitrary matrices,
\(\RR \langle x \rangle \) is no longer the natural algebra we should work with.

Let \(x = (x_1, \dots, x_g)\) be formal variables and let
\(x^* = (x_1^*, \dots, x_g^*)\) denote their formal adjoints. Once again, we let
the ring \(\RR \langle x,x^* \rangle \) be the finite formal sums of words in
\(x_1,x_1^*, \dots , x_g,x_g^*\) with coefficients in \(\RR \). Endow
\(\RR \langle x,x^* \rangle \) with an involution \(^*\) which sends
\(x_i \mapsto x_i^*\) and \(x_i^* \mapsto x_i\) and reverses the order of words
extended linearly. Notice that this involution behaves identically to the
adjoint with respect to products and sums of matrices. This new ring inherits a
natural functional calculus just like that in section \ref{sec:ExtMuliVarFun}
except it can accept \emph{any} matrix as an input instead of simply
self-adjoint matrices.

\begin{example}
  Let \(f(x,y) = x^*y - x y^* x +2\). Then
  \[
    f^*(x,y) = y^*x - x^*yx^* + 2.
  \]
  Evaluating \(f\) on a pair of non self-adjoint matrices is left to the reader.
\end{example}

\subsection{Matrices of nc Polynomials}

It is occasionally useful in the larger theory of free analysis (\eg{} when
construction the free topology in section \ref{sec:admtopo} and when
characterizing the zero sets of nc polynomials in \cref{ch:ZeroDiv}) to consider
matrices where the entries are nc polynomials. Formally,
let \(\RR \langle x \rangle ^{k\times k}\) denote the set of \(k \times k\) matrices
with entries in \(\RR \langle x \rangle \).\footnote{Some sources additionally
  consider non-square matrices but this is rare.}
We can naturally extend the involution \(^*\) on \(\RR \langle x \rangle \) to
our matrices by applying \(^*\) component wise and taking the transpose of the
matrix.\footnote{We could likewise define
  \(\RR \langle x,x^* \rangle\) and extend the corresponding involution.}

Given some \(\delta \in \RR \langle x \rangle ^{k \times k}\) a matrix of nc
polynomials, and \(X \in \HH_n^g\) there is a natural evaluation map.
\begin{align*}
  \varepsilon_\delta: {\HH_n}^g &\longrightarrow M_{nk}(\CC) \\
             X &\longmapsto \delta(X)
\end{align*}
given by evaluating each polynomial in \(\delta\) at \(X\) and then viewing the
result at a block \(k \times k\) where each block is an \(n \times n\) matrix.

\begin{example}
  Define \(\delta \in \RR \langle x,y \rangle ^{2\times 2}\) as
  \[
    \delta(x,y) = \begin{bmatrix}
               x^2-xyx+1 & xy-yx \\
               x^4 & y^3-5xy+3
        \end{bmatrix}.
  \]
  Then
  \[
    \delta^*(x,y) = \begin{bmatrix}
               x^2-xyx+1 & yx-xy \\
               x^4 & y^3-5yx+3
        \end{bmatrix}.
  \]
  For an evaluation, we will once again let
  \[
    X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad Y =\begin{bmatrix} 2&0\\0&0 \end{bmatrix}.
  \]
  We already know what the evaluations of the first column from examples
  \ref{ex:helton1} and \ref{ex:multivareval}, so we need only complute the
  second column.
  \begin{align*}
    XY-YX &= \begin{bmatrix} 0&-4\\4&0 \end{bmatrix}  \\
    \\
    Y^3-5XY+3 &= \begin{bmatrix} -29&0\\-20&3 \end{bmatrix}.
  \end{align*}
  And thus,
  \[
    \delta(X,Y) =
    \begin{bmatrix}
      -11&-4&0&-4 \\
      -4&1&4&0 \\
      164&120&-29&0\\
      120&84&-20&3
    \end{bmatrix}.
  \]
\end{example}
