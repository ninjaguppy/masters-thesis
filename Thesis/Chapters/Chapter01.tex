%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

\section{Functional Calculus}%
\label{sec:functionalcalc}

Functional Calculus refers to the process of extending the domain of a function
on \(\RR\) to include matrices (or in some cases operators). The most basic
formulation uses the fact that the space \(n \times n\) matrices forms a ring
and so there is a natural way to evaluate polynomials \(f \in \CC [x]\). If we
require that $A \in M_n(\CC)$ is self-adjoint---and hence diagonalizable as
$A = U \Lambda U^*$---then it is a standard result that:
\begin{align*}
  f(A) &= a_nA^n + \cdots + a_1A + a_0 \\
  &= a_n \left( U\Lambda U^* \right) ^n + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= a_n U\Lambda^n U^* + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= U \left( a_n\Lambda ^n + \cdots + a_1\Lambda + a_0 \right) U^* \\
  &= U \left( f(\Lambda) \right) U^*
\end{align*}
Further, since \(\Lambda\) is diagonal and $f$ is a polynomial,
\[
  f \left( \begin{bmatrix} \lambda_1 &  &  \\  & \ddots &  \\  &  & \lambda_n \end{bmatrix}  \right)
  = \begin{bmatrix} f(\lambda_1) &  &  \\  & \ddots &  \\  &  & f(\lambda_n) \end{bmatrix}
\]
Therefore, given a self-adjoint matrix \(A\) and a polynomial \(f \in \CC [x]\)
\[
  f(A) = Uf(\Lambda)U^* = U \;\text{diag}\{f(\lambda_{1}), \dots , f(\lambda_n)\} \; U^*
\]
Since self-adjoint matrices play such a vital role in free analysis, we will let
\(\HH_n \subset M_n(\CC)\) denote the set of \(n \times n\)-matrices over \(\CC \).
With the polynomial case in mind, we can extend a function \(g: [a,b] \to \CC \)
to a function on self adjoint {\color{red} (normal?)} matrices with their
spectrum in \([a,b]\). Let \(A\) be such a matrix (diagonalized by the unitary
matrix \(U\)), and define
\[
  g(A) = U
  \begin{bmatrix} g(\lambda_1) & &\\ &\ddots& \\ & & \lambda_n \end{bmatrix}
  U^*
\]
Thus, for each \(n \in \NN \), \(g\) induces a function on the self-adjoint
\(n \times n\) matrices with spectrum in \([a,b]\).  The natural ordering
{\color{red} {explain why natural?}} on self-adjoint matrices is called the
\textbf{Loewner Order}:

\begin{definition}[Loewner Ordering]
  For like size self-adjoint matrices, we say that \(A \preceq B\) if \(B - A \)
  is positive semidefinite and \(A \prec B\) is \(B-A\) is positive definite.
\end{definition}

With this ordering in place, we can extend many of the familiar function
theoretic properties (monotonicity, convexity) to these matrix-values functions.
In fact, these properties are defined identically to their classical counterpart:
We say that a function is \emph{matrix-monotone} if \(A \preceq B\) implies that
\(f(A) \preceq f(B)\) and \emph{matrix-convex} (or \emph{nc-convex}) if
\[
  f \left( \frac{X+Y}{2} \right) \preceq \frac{f(X)+f(Y)}{2}
\]
for every pair of like-size matrice for which \(f\) is defined. These condition
are rather restrictive (since the must hold for matrices of \emph{all} sizes) so
many functions which are convex/monotone (in the traditional sense) fail to be
matrix-convex/monotone. For example, \(f(x)=x^4\) fails to be nc-convex.
{\color{red} Below is an example from FCAC (Helton). What is the best way to
  refer to this?

Indeed, if
\[
  X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad \begin{bmatrix} 2&0\\0&0 \end{bmatrix}
\]
Then
\[
  \frac{X^4+Y^4}{2} - \left( \frac{1}{2}X +\frac{1}{2}Y \right) ^4
  = \begin{bmatrix} 164 &120\\120&84 \end{bmatrix}
\]
Which is not positive definite! Thus \(x^4\) fails to be convex on even
\(2\times 2\) matrices.
}

{\color{blue} nc-positive if positive for all matrices}

Further, a number of the standard constructions lift identically in this
functional calculus.
\begin{definition}[Directional Derivative]
  The derivative of \(f\) in the direction \(H\) is
  \[
    Df(X)[H] = \lim_{t \to 0} \frac{f(X+tH) - f(X)}{t}
  \]
  where \(H\) and \(X\) are like-size self-adjoint matices.
\end{definition}

Often, the best way to compute these directional derivatives is via an
equivalent formulation:
\[
    Df(X)[H] = \left.\frac{df(X+tH)}{dt}\right|_{t=0}
\]
This version allows us to more easily define higher order derivatives
\[
    D^{(k)}f(X)[H] = \left.\frac{d^{(k)}f(X+tH)}{d^{(k)}t}\right|_{t=0}
\]

Despite nc-convexity being so restrictive, Lemma 12 in \cite{heltonFree2013}
shows that the standard characterization of convexity via the second
derivative: a function \(f\) is convex if and only if \(D^{2}f(X)[H]\) is
nc-positive. Unlike the classical case, however, the only convex polynomials are
of degree 2\footnote{See \cite{heltonFree2013} for details.}.

\section{Extending Multi-Variable Functions}%
\label{sec:ExtMuliVarFun}

We can extend this same functional calculus to functions of several variables,
although the details are a bit more subtle. We could simply ``plug in'' at tuple
of matrices to a standard multivariable polynomial ring over \(\RR \) or
\(\CC \), but this ignores the noncommutativity of \(M_n(\CC)\). In light of this,
let \(x = (x_1 , \dots x_g)\) be a
\(g\)-tuples of noncommuting formal variables. The formal variables
\(x_1, \dots , x_n\) are \emph{free} in the sense that there are no nontrivial
relations between them.\footnote{This becomes important in the eventual
  functional calculus---matrices \emph{do} have nontrivial relations. See
  section [ALGEBRAIC CONSTRUCTION] for the details.}
A \textbf{word} in \(x\) is a product of these
variables (\eg \(x_1x_3x_1x_4^2\) or \(x_1^2x_5^3\)). An \textbf{nc-polynomial}
in \(x\) is a formal finite linear combination of words in \(x\) with
coefficients in your favorite field. We use \(\RR \langle x \rangle\) and
\(\CC \langle x \rangle\) to denote the set of nc-polynomails in \(x\) over
\(\RR \) or \(\CC \) respectively.

With \(\CC \langle x \rangle \) constructed, we can define the functional
calculus. Given a word \(w(x) = x_{i_1}^{p_1}\cdots x_{i_d}^{p_d}\) and a
\(g\)-tuple of self-adjoint matrices, \(X\), we can evaluate \(w\) on \(X\) via
\(w(X) = X_{i_1}^{p_1}\cdots X_{i_d}^{p_d}\). Since our nc-polynomials are
linear combinations of these words, we can extend this evaluation to evaluation
of entire polynomials. Algebriacly, we have a natural evaluation map:
Given some \(f \in \CC \langle x \rangle \) and
\(X = \left( X_1, \dots ,X_n \right) \) a
\(g\)-tuple of self-adjoint matrices, define
\begin{align*}
  \varepsilon_f: {\HH_n}^g &\longrightarrow M_n(\CC) \\
             X &\longmapsto f(X)
.\end{align*}

In the context of these multivariate functions, our definition of the
Directional Derivative still makes sense (although our direction \(H\) now
becomes a tuple of directions). We also inherit (from multi-variable calculus) a
notion of the \textbf{gradient} of a function---but this will require a bit more work.


\section{The Natural Involution on nc-Polynomials}%
\label{sec:NatInvo}

{\color{blue} Short section on the involution. Could also put this in the
  algebra errata}

\section{Matrix Universes}%
\label{sec:MatUniv}

{\color{blue} Be sure to include the ``dot product'' here}

\section{The Topology of Matrix Universes }%
\label{sec:TopManUniv}

{\color{blue} How much detail here? Just what we are doing or the basics of the
  other topologies? This could also talk about the nc varieties/singular sets}



\section{Tracial Functions and Uniqueness of the Gradient}%
\label{sec:TracGrad}
Now that we have \(\MM^d\), we can work with general functions on our matrix
universe. As a whole, free analysis is concerned with so-called \emph{free
  functions}, which respect the direct sums and unitary conjugation.
{\color{red} Do they need to be graded?}
\begin{definition}[Free Function]
\label{def:FreeFun}
  A function \(f: D\to \MM^{\color{red} \text{something}}\) is called \textbf{free} if
  \begin{enumerate}
    \item \(f(X\oplus Y)= f(X) \oplus f(Y)\)
    \item \(f(U X U^*) = f(U)f(X)f(U^*)\) where \(X\) and \(U\) are like-size
          and \(U\) is unitary.
  \end{enumerate}
\end{definition}

The two other classes of functions we are concerned with are those that act like
the trace and the determinant:
\begin{definition}[Determinantal Free Function]
  \label{def:DetFreeFun}
  A function \(f: D \to \CC \) is a \textbf{determinantal free function} if
  \begin{enumerate}
    \item \(f(X\oplus Y) = f(X)f(Y)\)
    \item \(f(U X U^*) = f(X)\) where \(X\) and \(U\) are like-size
          and \(U\) is unitary.
  \end{enumerate}
\end{definition}

\begin{definition}[Tracial Free Function]
  \label{def:TrFreeFun}
  A function \(f: D \to \CC \) is a \textbf{tracial free function} if
  \begin{enumerate}
    \item \(f(X\oplus Y) = f(X)+f(Y)\)
    \item \(f(U X U^*) = f(X)\) where \(X\) and \(U\) are like-size
          and \(U\) is unitary.
  \end{enumerate}
\end{definition}

It is only these tracial functions which inherit the gradient mentioned above.
Similarly to traditional multivariable calculus we define the gradient via its
relationship to the directional derivative:
\begin{definition}[Free Gradient]
\label{def:FreeGrad}
  Given a tracial free function \(f\), the free gradient, \(\nabla f\), is the
  unique free function satisfying
  \[
    \tr \left( H \cdot \nabla f(X) \right) = \tr\, Df(X)[H]
  \]
\end{definition}

It is not-at-all obvious that such a \(\nabla f \) should be unique---after all
any linear combination of commutator is has trace zero. {\color{red} should I
  explain this?} In the case that \(f\) is a single-variable function we can
replace \(\nabla f\) with the traditional derivative, \(f'\), as seen in
\cite[Thm 3.3]{pascoeTrace2020}.
\begin{theorem}
  Let \(f: (a,b)\to \RR \) be a \(C^1\) function. Then
  \[
    \tr \, Df(X)[H] = \tr \left( Hf'(X) \right)
  \]
\end{theorem}

The proof in \cite{pascoeTrace2020} simply asserts the uniqueness of a function
\(g(X)\) and then shows that \(g(x)=f'(x)\) for \(x \in (a,b)\). Instead, we can
construct such a \(g\) and recover the theorem along the way:
\begin{proof}

We start with a construction from Bhatia's Matrix Analysis: Let
$f \in C ^{1} (I)$ and define $f ^{[1]} $ on $I \times I$ by
\[
  f^{[1]} (\lambda,\mu) =
  \begin{cases}
    \frac{f(\lambda) - f(\mu)}{\lambda-\mu} & \lambda \neq \mu \\
    f'(\lambda) & \lambda = \mu.
  \end{cases}
\]
We call $f ^{[1]} (\lambda,\mu)$ the \emph{first divided difference} of $f$ at
$(\lambda,\mu)$. If $\Lambda$ is a diagonal matrix with entries
$\{ \lambda_{i}\} $, We may extend $f$ to accept $\Lambda$ by
defining the $(i,j)$-entry of $f ^{[1]} (\Lambda)$ to be
$f ^{[1]} (\lambda_i,\lambda_j)$. If $A$ is a self adjoint matrix with
$A = U \Lambda U ^{*} $, then we define
$f ^{[1]} (A) = U f ^{[1]} (\Lambda) U ^{*} $. Now we borrow a theorem from
Bhatia \cite{bhatiaMatrixAnalysis1997}:
\begin{theorem}[Bhatia V.3.3]

{\color{red} Theorem numbering?}
  Let $f \in C ^{1} (I)$ and let $A$ be a self adjoint matrix with all
  eigenvalues in $I$. Then \[
    Df(A)[H] = f ^{[1]} (A) \circ H,
  \]
  where $\circ$ denotes the Schur-product\footnote{Entrywise} in a basis where $A$ is diagonal.
\end{theorem}

That is, if $A = U   \Lambda U ^{*} $, then
\[
  Df(A)[H] = U \left( f ^{[1]} (\Lambda) \circ (U ^{* } H U) \right)U ^{*}.
\]
%
To prove our claim, we need to take the trace of both sides. Since trace is
invariant under a change of basis, it is clear that
\[
  \tr Df(A)[H] = \tr \left( f ^{[1]} (\Lambda) \circ (U ^{* } H U) \right).
\]
If $U = u_{ij}$, $U ^{*} = \overline{u}_{ij}$ and $H = h_{ij}$, then the
$(i,j)$-entry of $U ^{*}HU$ is
\[
  {(U ^{* } H U)}_{ij} = \overline{u}_{ik}h_{k\ell}u_{\ell j}
\]
Where we sum over the duplicate indices $k$ and $\ell$. While the structure of
$f ^{[1]} (\Lambda)$ is a bit unruly, our diagonal entries are $f'(\lambda)$.
This means that when we take the trace of the Schur product, we have
\[
 \sum_k\sum_\ell \sum_i f'(\lambda_i)\overline{u}_{ik}h_{k\ell}u_{\ell i}
\]
Now consider the matrix product
$U\, \text{diag} \{f'(\lambda_1), \dots ,f'(\lambda_n)\} \,U ^{*} H $. Since one of our terms
is diagonal, the trace of this multiplication is simple:
\[
  \text{tr}\; U \,\text{diag} \{f'(\lambda_1), \dots ,f'(\lambda_n)\}\, U ^{*} H
  = \sum_k\sum_\ell\sum_i  u_{ik}f'(\lambda_k) \overline{u}_{k \ell} h_{\ell i}
\]
Since \(u_{ik}, \overline{u}_{k\ell}, h_{\ell i} \in \CC \) they commute. We can
then relabel our indices
$i \mapsto \ell\; \ell \mapsto k \; k \mapsto i $ to get
\[
  \tr\; U \,\text{diag} \{f'(\lambda_1), \dots ,f'(\lambda_n)\}\, U ^{*} H
  = \sum_k\sum_\ell \sum_i f'(\lambda_i) \overline{u}_{i k} h_{k \ell}u_{\ell i},
\]
So, for every direction \(H\), we have that
$\tr \left( U\, \text{diag} \{f'(\lambda_1), \dots ,f'(\lambda_n)\} \,U ^{*} H\right) =
   \tr \left( f ^{[1]} (\Lambda) \circ (U ^{* } H U) \right). $
{\color{red} overfull hbox :eyeroll:}
By picking the ``correct'' \(H\)\footnote{See example EXAMPLE NUMBER for
  details}, we conclude that our unique quantity \(g(X)\) is
\(U\, \text{diag} \{f'(\lambda_1), \dots ,f'(\lambda_n)\} \,U ^{*} \). But,
recall that \(X=U\Lambda U\) so, in the functional calculus, $g(X) = f'(X)$.
This recovers theorem 3.3 of \cite{pascoeTrace2020} as we have constructed a
\(g\) such that
\[
  \tr \; Df(X)[H] = \tr H g(X)
\]
\end{proof}

{\color{blue} Connect this to the gradient and this section is done}
