%************************************************
\chapter{A First Attempt}\label{ch:FirstAttempt}
%************************************************


\section{Introduction}%
\label{sec:Intro}

As a note to the reader (and myself): things written in {\color{blue} blue}
denote things I want to add/expand upon and things writteng in {\color{red} red}
denote things that I need to add/find out/fix

{\color{blue} Gotta do this at some point. Maybe here we define things like \(\mathcal{U}_n\).}

\section{Functional Calculus}%
\label{sec:functionalcalc}

Functional Calculus refers to the process of extending the domain of a function
on \(\RR\) to include matrices (or in some cases operators). The most basic
formulation uses the fact that the space \(n \times n\) matrices forms a ring
and so there is a natural way to evaluate polynomials \(f \in \CC [x]\). If we
require that $A \in M_n(\CC)$ is self-adjoint---and hence diagonalizable as
$A = U \Lambda U^*$---then it is a standard result that:
\begin{align*}
  f(A) &= a_nA^n + \cdots + a_1A + a_0 \\
  &= a_n \left( U\Lambda U^* \right) ^n + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= a_n U\Lambda^n U^* + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= U \left( a_n\Lambda ^n + \cdots + a_1\Lambda + a_0 \right) U^* \\
  &= U \left( f(\Lambda) \right) U^*
\end{align*}
Further, since \(\Lambda\) is diagonal and $f$ is a polynomial,
\[
  f \left( \begin{bmatrix} \lambda_1 &  &  \\  & \ddots &  \\  &  & \lambda_n \end{bmatrix}  \right)
  = \begin{bmatrix} f(\lambda_1) &  &  \\  & \ddots &  \\  &  & f(\lambda_n) \end{bmatrix}
\]
Therefore, given a self-adjoint matrix \(A\) and a polynomial \(f \in \CC [x]\)
\[
  f(A) = Uf(\Lambda)U^* = U \;\text{diag}\{f(\lambda_{1}), \dots , f(\lambda_n)\} \; U^*
\]
Since self-adjoint matrices play such a vital role in free analysis, we will let
\(\HH_n \subset M_n(\CC)\) denote the set of \(n \times n\)-matrices over \(\CC \).
With the polynomial case in mind, we can extend a function \(g: [a,b] \to \CC \)
to a function on self adjoint matrices with their
spectrum in \([a,b]\). Let \(A\) be such a matrix (diagonalized by the unitary
matrix \(U\)), and define
\[
  g(A) = U
  \begin{bmatrix} g(\lambda_1) & &\\ &\ddots& \\ & & \lambda_n \end{bmatrix}
  U^*
\]
Thus, for each \(n \in \NN \), \(g\) induces a function on the self-adjoint
\(n \times n\) matrices with spectrum in \([a,b]\).  The natural ordering
 on self-adjoint matrices is called the
\textbf{Loewner Order}:

\begin{definition}[Loewner Ordering]
\label{def:LoewnerOrder}
  For like size self-adjoint matrices, we say that \(A \preceq B\) if \(B - A \)
  is positive semidefinite and \(A \prec B\) is \(B-A\) is positive definite.
\end{definition}

With this ordering in place, we can extend many of the familiar function
theoretic properties (monotonicity, convexity) to these matrix-values functions.
In fact, these properties are defined identically to their classical counterpart:
We say that a function is \emph{matrix-monotone} if \(A \preceq B\) implies that
\(f(A) \preceq f(B)\) and \emph{matrix-convex} (or \emph{nc-convex}) if
\[
  f \left( \frac{X+Y}{2} \right) \preceq \frac{f(X)+f(Y)}{2}
\]
for every pair of like-size matrice for which \(f\) is defined. These condition
are rather restrictive (since the must hold for matrices of \emph{all} sizes) so
many functions which are convex/monotone (in the traditional sense) fail to be
matrix-convex/monotone. For a full treatment of nc-convexity, see
\cite{heltonFree2013}. To illustrate the restrictiveness of nc-convexity,
{\color{red} we will steal an example from Helton. (I don't like this phrasing. I don't
  mind the word steal, its just awkward)}
In contrast to the real (or even complex) case, \(f(x)=x^4\) fails to be nc-convex.
Indeed, if
\[
  X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad \begin{bmatrix} 2&0\\0&0 \end{bmatrix}
\]
Then
\[
  \frac{X^4+Y^4}{2} - \left( \frac{1}{2}X +\frac{1}{2}Y \right) ^4
  = \begin{bmatrix} 164 &120\\120&84 \end{bmatrix}
\]
Which is not positive definite! Thus \(x^4\) fails to be convex on even
\(2\times 2\) matrices.

Further, a number of the standard constructions lift identically in this
functional calculus.
\begin{definition}[Directional Derivative]
  \label{def:DirDeriv}
  The derivative of \(f\) in the direction \(H\) is
  \[
    Df(X)[H] = \lim_{t \to 0} \frac{f(X+tH) - f(X)}{t}
  \]
  where \(H\) and \(X\) are like-size self-adjoint matices.
\end{definition}

Often, the best way to compute these directional derivatives is via an
equivalent formulation:
\[
    Df(X)[H] = \left.\frac{df(X+tH)}{dt}\right|_{t=0}
\]
This version allows us to more easily define higher order derivatives
\[
    D^{(k)}f(X)[H] = \left.\frac{d^{(k)}f(X+tH)}{d^{(k)}t}\right|_{t=0}
\]

Just as in the classical case, the second derivative tells gives us information
about the convexity of a function. A function \(f:M_n(\CC)\to M_n(\CC)\) is said
to be \textbf{positive} if \(0\preceq A \implies 0\preceq f(A)\). In the
functional calculus, we say that \(f\) is \textbf{nc positive} if it is positive
as a map on \(M_n(\CC)\) for all \(n\).
Despite nc-convexity being so restrictive, Lemma 12 in \cite{heltonFree2013}
shows that the standard characterization of convexity via the second
derivative: a function \(f\) is convex if and only if \(D^{2}f(X)[H]\) is
nc-positive. Unlike the classical case, however, the only convex polynomials are
of degree 2.\footnote{See \cite{heltonFree2013} for details.}

\section{Extending Multi-Variable Functions}%
\label{sec:ExtMuliVarFun}

We can extend this same functional calculus to functions of several variables,
although the details are a bit more subtle. We could simply ``plug in'' at tuple
of matrices to a standard multivariable polynomial ring over \(\RR \) or
\(\CC \), but this ignores the noncommutativity of \(M_n(\CC)\). In light of this,
let \(x = (x_1 , \dots x_g)\) be a
\(g\)-tuples of noncommuting formal variables. The formal variables
\(x_1, \dots , x_n\) are \emph{free} in the sense that there are no nontrivial
relations between them.\footnote{This becomes important in the eventual
  functional calculus---matrices \emph{do} have nontrivial relations. See
  section [ALGEBRAIC CONSTRUCTION] for the details.}
A \textbf{word} in \(x\) is a product of these
variables (\eg \(x_1x_3x_1x_4^2\) or \(x_1^2x_5^3\)). An \textbf{nc-polynomial}
in \(x\) is a formal finite linear combination of words in \(x\) with
coefficients in your favorite field. We use \(\RR \langle x \rangle\) and
\(\CC \langle x \rangle\) to denote the set of nc-polynomials in \(x\) over
\(\RR \) or \(\CC \) respectively.

With \(\CC \langle x \rangle \) constructed, we can define the functional
calculus. Given a word \(w(x) = x_{i_1}^{p_1}\cdots x_{i_d}^{p_d}\) and a
\(g\)-tuple of self-adjoint matrices, \(X\), we can evaluate \(w\) on \(X\) via
\(w(X) = X_{i_1}^{p_1}\cdots X_{i_d}^{p_d}\). Since our nc-polynomials are
linear combinations of these words, we can extend this evaluation to evaluation
of entire polynomials. Algebraically, we have a natural evaluation map:
Given some \(f \in \CC \langle x \rangle \) and
\(X = \left( X_1, \dots ,X_g \right) \) a
\(g\)-tuple of self-adjoint matrices, define
\begin{align*}
  \varepsilon_f: {\HH_\bullet}^g &\longrightarrow M_\bullet(\CC) \\
             X &\longmapsto f(X)
.\end{align*}
Notice that our functions are \textbf{graded} in the sense that if \(X\) is a
tuple of \(n \times n\) matrices, then \(f(X)\) is also a tuple of
\(n \times n\) matrices.

In the context of these multivariate functions, our definition of the
Directional Derivative still makes sense (although our direction \(H\) now
becomes a tuple of directions). We also inherit (from multi-variable calculus) a
notion of the \textbf{gradient} of a function---but this will require a bit more work.

{\color{blue} Maybe an example of an evaluation?}

\subsection{The Natural Involution on nc-Polynomials}%
\label{ssec:NatInvo}

Given our ring of nc polynomials, we may define an involution \(^*\) which we
may view as an extension of the conjugate transpose. Let \(^*\) reverse the
order of words (\ie \((x_1x_3x_2^2)^* = x_2^2x_3x_1\)) and extend linearly to
all of \(\RR \langle x \rangle \). We consider the formal
variables \(x_1, \dots , x_n\) \emph{symmetric} in the sense that
\(x_i^* = x_i\). We say that a polynomial \(p \in \RR \langle x \rangle \) is
symmetric if \(p^* = p\). For example, if
\[
  p(x) = 5x_1^2x_3x_2 + x_3x_2x_3 \qquad q(x) = 3x_2x_1x_2 + x_3^2 - x_1 ,
\]
then a cursory inspection tells that \(q\) is symmetric while \(p\) is not.

Notice that the majority of the previous two sections breaks down if we try to
extend functions to non-self-adjoint matrices. While the idea of ``plugging in''
a tuple of matrices so some element of \(\RR \langle x \rangle \) via the same
functional calculus described above, but we can actually \emph{add} structure to
\(\RR \langle x \rangle \) and made evaluation more natural.

Let \(x = (x_1, \dots, x_g)\) be formal variables and let
\(x^* = (x_1^*, \dots, x_g^*)\) denote their formal adjoints. Once again, we let
the ring \(\RR \langle x,x^* \rangle \) be the finite formal sums of words in
\(x_1,x_1^*, \dots , x_g,x_g^*\) with coefficients in \(\RR \). Endow
\(\RR \langle x,x^* \rangle \) with an involution \(^*\) which sends
\(x_i \mapsto x_i^*\) and \(x_i^* \mapsto x_i\) and reverses the order of words
extended linearly. Notice that this involution behaves identically to the
adjoint with respect to products and sums of matrices. This new ring inherits a
natural functional calculus just like that in section \ref{sec:ExtMuliVarFun}
except it can accept \emph{any} matrix as an input instead of simply
self-adjoint matrices.

\subsection{Matrices of nc-Polynomials}

It is occasionally useful in the larger theory of free analysis (\eg when
construction the free topology in section \ref{sec:admtopo}) to consider
matrices where the matrices are nc polynomials. Formally,
let \(\RR \langle x \rangle ^{k\times k}\) denote the set of \(k \times k\) matrices
with entries in \(\RR \langle x \rangle \).\footnote{Some sources additionally
  consider non-square matrices but this is rare.}
We can naturally extend the involution \(^*\) on \(\RR \langle x \rangle \) to
our matrices by applying \(^*\) component wise and taking the transpose of the matrix.

{\color{blue} Another example here---some matrix and its ``adjoint''}

Given some \(\delta \in \RR \langle x \rangle ^{k \times k}\) a matrix of nc
polynomials, and \(X \in \HH_n^g\) there is a natural evaluation map.
\begin{align*}
  \varepsilon_\delta: {\HH_n}^g &\longrightarrow M_{nk}(\CC) \\
             X &\longmapsto \delta(X)
\end{align*}
given by evaluating each polynomial in \(\delta\) at \(X\) and then viewing the
result at a block \(k \times k\) where each block is an \(n \times n\) matrix.

{\color{blue} Definitely an example here.}
