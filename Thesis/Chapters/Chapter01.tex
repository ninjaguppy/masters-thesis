%************************************************
\chapter{A First Attempt}\label{ch:FirstAttempt}
%************************************************

\section{Functional Calculus}%
\label{sec:functionalcalc}

Functional Calculus refers to the process of extending the domain of a function
on \(\RR\) to include matrices (or in some cases operators). The most basic
formulation uses the fact that the space \(n \times n\) matrices forms a ring
and so there is a natural way to evaluate polynomials \(f \in \CC [x]\). If we
require that $A \in M_n(\CC)$ is self-adjoint---and hence diagonalizable as
$A = U \Lambda U^*$---then it is a standard result that:
\begin{align*}
  f(A) &= a_nA^n + \cdots + a_1A + a_0 \\
  &= a_n \left( U\Lambda U^* \right) ^n + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= a_n U\Lambda^n U^* + \cdots + a_1 U\Lambda U^* + a_0 \\
  &= U \left( a_n\Lambda ^n + \cdots + a_1\Lambda + a_0 \right) U^* \\
  &= U \left( f(\Lambda) \right) U^*
\end{align*}
Further, since \(\Lambda\) is diagonal and $f$ is a polynomial,
\[
  f \left( \begin{bmatrix} \lambda_1 &  &  \\  & \ddots &  \\  &  & \lambda_n \end{bmatrix}  \right)
  = \begin{bmatrix} f(\lambda_1) &  &  \\  & \ddots &  \\  &  & f(\lambda_n) \end{bmatrix}
\]
Therefore, given a self-adjoint matrix \(A\) and a polynomial \(f \in \CC [x]\)
\[
  f(A) = Uf(\Lambda)U^* = U \;\text{diag}\{f(\lambda_{1}), \dots , f(\lambda_n)\} \; U^*
\]
Since self-adjoint matrices play such a vital role in free analysis, we will let
\(\HH_n \subset M_n(\CC)\) denote the set of \(n \times n\)-matrices over \(\CC \).
With the polynomial case in mind, we can extend a function \(g: [a,b] \to \CC \)
to a function on self adjoint {\color{red} (normal?)} matrices with their
spectrum in \([a,b]\). Let \(A\) be such a matrix (diagonalized by the unitary
matrix \(U\)), and define
\[
  g(A) = U
  \begin{bmatrix} g(\lambda_1) & &\\ &\ddots& \\ & & \lambda_n \end{bmatrix}
  U^*
\]
Thus, for each \(n \in \NN \), \(g\) induces a function on the self-adjoint
\(n \times n\) matrices with spectrum in \([a,b]\).  The natural ordering
{\color{red} {explain why natural?}} on self-adjoint matrices is called the
\textbf{Loewner Order}:

\begin{definition}[Loewner Ordering]
\label{def:LoewnerOrder}
  For like size self-adjoint matrices, we say that \(A \preceq B\) if \(B - A \)
  is positive semidefinite and \(A \prec B\) is \(B-A\) is positive definite.
\end{definition}

With this ordering in place, we can extend many of the familiar function
theoretic properties (monotonicity, convexity) to these matrix-values functions.
In fact, these properties are defined identically to their classical counterpart:
We say that a function is \emph{matrix-monotone} if \(A \preceq B\) implies that
\(f(A) \preceq f(B)\) and \emph{matrix-convex} (or \emph{nc-convex}) if
\[
  f \left( \frac{X+Y}{2} \right) \preceq \frac{f(X)+f(Y)}{2}
\]
for every pair of like-size matrice for which \(f\) is defined. These condition
are rather restrictive (since the must hold for matrices of \emph{all} sizes) so
many functions which are convex/monotone (in the traditional sense) fail to be
matrix-convex/monotone. For example, \(f(x)=x^4\) fails to be nc-convex.
{\color{red} Below is an example from FCAC (Helton). What is the best way to
  refer to this?

Indeed, if
\[
  X = \begin{bmatrix} 4 &2\\2&2 \end{bmatrix}  \qquad \text{ and } \qquad \begin{bmatrix} 2&0\\0&0 \end{bmatrix}
\]
Then
\[
  \frac{X^4+Y^4}{2} - \left( \frac{1}{2}X +\frac{1}{2}Y \right) ^4
  = \begin{bmatrix} 164 &120\\120&84 \end{bmatrix}
\]
Which is not positive definite! Thus \(x^4\) fails to be convex on even
\(2\times 2\) matrices.
}

{\color{blue} nc-positive if positive for all matrices}

Further, a number of the standard constructions lift identically in this
functional calculus.
\begin{definition}[Directional Derivative]
  \label{def:DirDeriv}
  The derivative of \(f\) in the direction \(H\) is
  \[
    Df(X)[H] = \lim_{t \to 0} \frac{f(X+tH) - f(X)}{t}
  \]
  where \(H\) and \(X\) are like-size self-adjoint matices.
\end{definition}

Often, the best way to compute these directional derivatives is via an
equivalent formulation:
\[
    Df(X)[H] = \left.\frac{df(X+tH)}{dt}\right|_{t=0}
\]
This version allows us to more easily define higher order derivatives
\[
    D^{(k)}f(X)[H] = \left.\frac{d^{(k)}f(X+tH)}{d^{(k)}t}\right|_{t=0}
\]

Despite nc-convexity being so restrictive, Lemma 12 in \cite{heltonFree2013}
shows that the standard characterization of convexity via the second
derivative: a function \(f\) is convex if and only if \(D^{2}f(X)[H]\) is
nc-positive. Unlike the classical case, however, the only convex polynomials are
of degree 2\footnote{See \cite{heltonFree2013} for details.}.

\section{Extending Multi-Variable Functions}%
\label{sec:ExtMuliVarFun}

We can extend this same functional calculus to functions of several variables,
although the details are a bit more subtle. We could simply ``plug in'' at tuple
of matrices to a standard multivariable polynomial ring over \(\RR \) or
\(\CC \), but this ignores the noncommutativity of \(M_n(\CC)\). In light of this,
let \(x = (x_1 , \dots x_g)\) be a
\(g\)-tuples of noncommuting formal variables. The formal variables
\(x_1, \dots , x_n\) are \emph{free} in the sense that there are no nontrivial
relations between them.\footnote{This becomes important in the eventual
  functional calculus---matrices \emph{do} have nontrivial relations. See
  section [ALGEBRAIC CONSTRUCTION] for the details.}
A \textbf{word} in \(x\) is a product of these
variables (\eg \(x_1x_3x_1x_4^2\) or \(x_1^2x_5^3\)). An \textbf{nc-polynomial}
in \(x\) is a formal finite linear combination of words in \(x\) with
coefficients in your favorite field. We use \(\RR \langle x \rangle\) and
\(\CC \langle x \rangle\) to denote the set of nc-polynomails in \(x\) over
\(\RR \) or \(\CC \) respectively.

With \(\CC \langle x \rangle \) constructed, we can define the functional
calculus. Given a word \(w(x) = x_{i_1}^{p_1}\cdots x_{i_d}^{p_d}\) and a
\(g\)-tuple of self-adjoint matrices, \(X\), we can evaluate \(w\) on \(X\) via
\(w(X) = X_{i_1}^{p_1}\cdots X_{i_d}^{p_d}\). Since our nc-polynomials are
linear combinations of these words, we can extend this evaluation to evaluation
of entire polynomials. Algebriacly, we have a natural evaluation map:
Given some \(f \in \CC \langle x \rangle \) and
\(X = \left( X_1, \dots ,X_n \right) \) a
\(g\)-tuple of self-adjoint matrices, define
\begin{align*}
  \varepsilon_f: {\HH_n}^g &\longrightarrow M_n(\CC) \\
             X &\longmapsto f(X)
.\end{align*}

In the context of these multivariate functions, our definition of the
Directional Derivative still makes sense (although our direction \(H\) now
becomes a tuple of directions). We also inherit (from multi-variable calculus) a
notion of the \textbf{gradient} of a function---but this will require a bit more work.


\section{The Natural Involution on nc-Polynomials}%
\label{sec:NatInvo}

{\color{blue} Short section on the involution. Could also put this in the
  algebra errata}
